{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Postdam PM2.5 Linear Regression Forcasting \n",
    "\n",
    "* With Ridge And Lasso Regression Models\n",
    "* Between 2013 and 2023, data collected by DEBB021 was used.\n",
    "* To increase the accuracy of PM2.5 data estimation, NO2, O3, SO2, PM10 pollutant gas data accepted by the EEA was added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "os.chdir('..')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd\n",
    "import src.model_base as mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "* Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/CLEAN_MERGED_DE_DEBB021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/6n/b_b3xm5x4zxbgc671mk_m5440000gn/T/ipykernel_4706/672861493.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/CLEAN_MERGED_DE_DEBB021.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \"\"\"\n\u001B[0;32m--> 222\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/CLEAN_MERGED_DE_DEBB021.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/CLEAN_MERGED_DE_DEBB021.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.set_index('Start_Timestamp', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "* Defining Target and feature variables\n",
    "* Scale Features\n",
    "* Apply PCA\n",
    "* Split Data into Train, Validation, Test data\n",
    "* Train Linear Reg Model\n",
    "* Predict\n",
    "* Evaluate Validation and Test data\n",
    "* Hyperparameter Tuning ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Target and feature variables\n",
    "features = ['NO2-Value', 'O3-Value', 'SO2-Value', 'PM10-Value']\n",
    "target = 'PM2.5-Value'\n",
    "\n",
    "# Separate the features and target\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data \n",
    "\n",
    "Train, Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the indices for the splits\n",
    "train_end = int(len(df) * train_ratio)\n",
    "validation_end = train_end + int(len(df) * validation_ratio)\n",
    "\n",
    "# Split the dataset\n",
    "train_data = df[:train_end]\n",
    "validation_data = df[train_end:validation_end]\n",
    "test_data = df[validation_end:]\n",
    "\n",
    "\n",
    "print(f\"Training set size: {train_data.shape[0]}\")\n",
    "print(f\"Validation set size: {validation_data.shape[0]}\")\n",
    "print(f\"Test set size: {test_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[features])  # Fit only on training data\n",
    "\n",
    "# Scale the datasets\n",
    "X_train_scaled = scaler.transform(train_data[features])\n",
    "X_val_scaled = scaler.transform(validation_data[features])\n",
    "X_test_scaled = scaler.transform(test_data[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)\n",
    "Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA on the scaled data\n",
    "pca = PCA(n_components=0.95)  # Adjust based on the explained variance\n",
    "pca.fit(X_train_scaled)  # Fit only on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the datasets using the fitted PCA\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target variable\n",
    "y_train = train_data[target]\n",
    "y_val = validation_data[target]\n",
    "y_test = test_data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "* Initialize Linear Regression Model\n",
    "* Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "## With Validation Data\n",
    "\n",
    "Error metrics MAE, MSE, RMSE, MASE, MAPE\n",
    "\n",
    "* Regarding the MASE metric, calculating it requires a baseline prediction model for the time series, which is typically done by using the last observed value to predict the next (in the simplest case) or using more complex methods like ARIMA for one-step ahead forecasting. This is not included in the above script as it would require additional steps to implement the naive forecasting method for a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val_pca)\n",
    "\n",
    "print(y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metric\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation MAE: {mae_val}\")\n",
    "print(f\"Validation MSE: {mse_val}\")\n",
    "print(f\"Validation RMSE: {rmse_val}\")\n",
    "print(f\"Validation MAPE: {mape_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASE Calculation\n",
    "\n",
    "# Create naive forecasts for validation and test sets\n",
    "y_val_naive = np.roll(y_val, 1)\n",
    "\n",
    "# Calculate MAE for the naive forecasts\n",
    "mae_val_naive = mean_absolute_error(y_val[1:], y_val_naive[1:])\n",
    "\n",
    "# Calculate MASE for validation and test sets\n",
    "mase_val = mae_val / mae_val_naive\n",
    "\n",
    "print(f\"Validation MASE: {mase_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Calculate and print the test metrics\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")\n",
    "print(f\"Test MAPE: {mape_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASE Calculation\n",
    "\n",
    "# Create naive forecasts for validation and test sets\n",
    "y_test_naive = np.roll(y_test, 1)\n",
    "\n",
    "# Calculate MAE for the naive forecasts\n",
    "mae_test_naive = mean_absolute_error(y_test[1:], y_test_naive[1:])\n",
    "\n",
    "# Calculate MASE for validation and test sets\n",
    "mase_test = mae_test / mae_test_naive\n",
    "\n",
    "print(f\"Test MASE: {mase_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's assume y_val is your validation set actual values and y_val_pred are the predictions\n",
    "# Similarly, y_test and y_test_pred for the test set\n",
    "\n",
    "# Plotting the actual vs predicted values for validation set\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual values - using blue color with a line marker\n",
    "plt.plot(validation_data.index, y_val, color='blue', marker='o', label='Actual', linestyle='-', linewidth=1)\n",
    "\n",
    "# Predicted values - using red color with a cross marker\n",
    "plt.plot(validation_data.index, y_val_pred, color='red', marker='x', label='Predicted', linestyle='None')\n",
    "\n",
    "plt.title('Validation Set - Actual vs Predicted PM2.5')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# For the test set, do similarly\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual values - using blue color with a line marker\n",
    "plt.plot(test_data.index, y_test, color='blue', marker='o', label='Actual', linestyle='-', linewidth=1)\n",
    "\n",
    "# Predicted values - using red color with a cross marker\n",
    "plt.plot(test_data.index, y_test_pred, color='red', marker='x', label='Predicted', linestyle='None')\n",
    "\n",
    "plt.title('Test Set - Actual vs Predicted PM2.5')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperPramater Tuning\n",
    "\n",
    "Linear Regression typically has fewer hyperparameters than other models like neural networks or ensemble models. However, there are still some aspects of the model that you can adjust. For instance, you can apply regularization, which can be considered a form of hyperparameter tuning. The most common types of regularized linear regression are Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression (L2)\n",
    "Ridge regression, also known as Tikhonov regularization, is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated. In multicollinear data, least-squares estimates are unbiased, but their variances are large, so they may be far from the true value. Ridge regression adds a degree of bias to the regression estimates, which often results in a decrease in the standard errors.\n",
    "\n",
    "* RandomizedSearchCV is a useful alternative to GridSearchCV when the parameter space is large. It samples a fixed number of parameter combinations from the specified distributions, which can be much more efficient, especially when some hyperparameters do not influence the performance of the model significantly.\n",
    "\n",
    "\n",
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Ridge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distribution of hyperparameters to sample from\n",
    "# Here, we define a uniform distribution over a log scale for the alpha parameter\n",
    "param_distributions = {'alpha': uniform(1e-4, 1e4)}\n",
    "\n",
    "# Set up the random search with cross-validation\n",
    "# n_iter sets the number of different combinations to try\n",
    "# cv is the number of folds for cross-validation\n",
    "# scoring determines the metric used to evaluate the models\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=100, cv=5, scoring='neg_mean_absolute_error', verbose=1, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search to the data (use the PCA-transformed data)\n",
    "random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters found: ', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best estimator to make predictions\n",
    "y_val_pred = random_search.best_estimator_.predict(X_val_pca)\n",
    "y_test_pred = random_search.best_estimator_.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set\n",
    "\n",
    "# Error Metric\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation MAE: {mae_val}\")\n",
    "print(f\"Validation MSE: {mse_val}\")\n",
    "print(f\"Validation RMSE: {rmse_val}\")\n",
    "print(f\"Validation MAPE: {mape_val}\")\n",
    "\n",
    "# MASE Calculation\n",
    "\n",
    "# Create naive forecasts for validation and test sets\n",
    "y_val_naive = np.roll(y_val, 1)\n",
    "\n",
    "# Calculate MAE for the naive forecasts\n",
    "mae_val_naive = mean_absolute_error(y_val[1:], y_val_naive[1:])\n",
    "\n",
    "# Calculate MASE for validation and test sets\n",
    "mase_val = mae_val / mae_val_naive\n",
    "\n",
    "print(f\"Validation MASE: {mase_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "# Calculate and print the test metrics\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")\n",
    "print(f\"Test MAPE: {mape_test}\")\n",
    "\n",
    "# MASE Calculation\n",
    "\n",
    "# Create naive forecasts for validation and test sets\n",
    "y_test_naive = np.roll(y_test, 1)\n",
    "\n",
    "# Calculate MAE for the naive forecasts\n",
    "mae_test_naive = mean_absolute_error(y_test[1:], y_test_naive[1:])\n",
    "\n",
    "# Calculate MASE for validation and test sets\n",
    "mase_test = mae_test / mae_test_naive\n",
    "\n",
    "print(f\"Test MASE: {mase_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's assume y_val is your validation set actual values and y_val_pred are the predictions\n",
    "# Similarly, y_test and y_test_pred for the test set\n",
    "\n",
    "# Plotting the actual vs predicted values for validation set\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual values - using blue color with a line marker\n",
    "plt.plot(validation_data.index, y_val, color='blue', marker='o', label='Actual', linestyle='-', linewidth=1)\n",
    "\n",
    "# Predicted values - using red color with a cross marker\n",
    "plt.plot(validation_data.index, y_val_pred, color='red', marker='x', label='Predicted', linestyle='None')\n",
    "\n",
    "plt.title('Validation Set - Actual vs Predicted PM2.5')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# For the test set, do similarly\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual values - using blue color with a line marker\n",
    "plt.plot(test_data.index, y_test, color='blue', marker='o', label='Actual', linestyle='-', linewidth=1)\n",
    "\n",
    "# Predicted values - using red color with a cross marker\n",
    "plt.plot(test_data.index, y_test_pred, color='red', marker='x', label='Predicted', linestyle='None')\n",
    "\n",
    "plt.title('Test Set - Actual vs Predicted PM2.5')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1)\n",
    "\n",
    "Lasso regression, which stands for Least Absolute Shrinkage and Selection Operator, is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e., models with fewer parameters). This is particularly useful when you have a large number of predictors because it automatically performs variable selection.\n",
    "\n",
    "The key difference between ridge regression and lasso regression lies in the penalty term added to the cost function. While ridge regression adds a squared magnitude of the coefficient as a penalty term to the cost function, lasso regression adds the absolute value of the magnitude of the coefficient as the penalty term.\n",
    "\n",
    "The advantages of lasso regression include:\n",
    "\n",
    "* It can produce simpler and more interpretable models due to feature selection.\n",
    "* It is less prone to overfitting as it penalizes the absolute size of the coefficients.\n",
    "* It can handle complex scenarios with many variables or features in the dataset.\n",
    "\n",
    "However, lasso regression can also have disadvantages, such as biasing estimates towards zero and sometimes underperforming when all variables are included in the true model. It can also struggle with grouped variables, where it tends to select only one variable from a group and ignore the others.\n",
    "\n",
    "### Model Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Lasso(max_iter=10000)  # Increased max_iter for convergence with high-dimensional data\n",
    "\n",
    "# Define the distribution of hyperparameters to sample from\n",
    "# loguniform is useful for alpha because it spans several orders of magnitude\n",
    "param_distributions = {'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "# Set up the random search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search to the data (use the PCA-transformed data)\n",
    "random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters found: ', random_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "y_val_pred = random_search.best_estimator_.predict(X_val_pca)\n",
    "y_test_pred = random_search.best_estimator_.predict(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set\n",
    "\n",
    "# Error Metric\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation MAE: {mae_val}\")\n",
    "print(f\"Validation MSE: {mse_val}\")\n",
    "print(f\"Validation RMSE: {rmse_val}\")\n",
    "print(f\"Validation MAPE: {mape_val}\")\n",
    "\n",
    "# MASE Calculation\n",
    "\n",
    "# Create naive forecasts for validation and test sets\n",
    "y_val_naive = np.roll(y_val, 1)\n",
    "\n",
    "# Calculate MAE for the naive forecasts\n",
    "mae_val_naive = mean_absolute_error(y_val[1:], y_val_naive[1:])\n",
    "\n",
    "# Calculate MASE for validation and test sets\n",
    "mase_val = mae_val / mae_val_naive\n",
    "\n",
    "print(f\"Validation MASE: {mase_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "# Calculate and print the test metrics\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")\n",
    "print(f\"Test MAPE: {mape_test}\")\n",
    "\n",
    "# MASE Calculation\n",
    "\n",
    "# Create naive forecasts for validation and test sets\n",
    "y_test_naive = np.roll(y_test, 1)\n",
    "\n",
    "# Calculate MAE for the naive forecasts\n",
    "mae_test_naive = mean_absolute_error(y_test[1:], y_test_naive[1:])\n",
    "\n",
    "# Calculate MASE for validation and test sets\n",
    "mase_test = mae_test / mae_test_naive\n",
    "\n",
    "print(f\"Test MASE: {mase_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume y_val is your validation set actual values and y_val_pred are the predictions\n",
    "# Similarly, y_test and y_test_pred for the test set\n",
    "\n",
    "# Plotting the actual vs predicted values for validation set\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual values - using blue color with a line marker\n",
    "plt.plot(validation_data.index, y_val, color='blue', marker='o', label='Actual', linestyle='-', linewidth=1)\n",
    "\n",
    "# Predicted values - using red color with a cross marker\n",
    "plt.plot(validation_data.index, y_val_pred, color='red', marker='x', label='Predicted', linestyle='None')\n",
    "\n",
    "plt.title('Validation Set - Actual vs Predicted PM2.5')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# For the test set, do similarly\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual values - using blue color with a line marker\n",
    "plt.plot(test_data.index, y_test, color='blue', marker='o', label='Actual', linestyle='-', linewidth=1)\n",
    "\n",
    "# Predicted values - using red color with a cross marker\n",
    "plt.plot(test_data.index, y_test_pred, color='red', marker='x', label='Predicted', linestyle='None')\n",
    "\n",
    "plt.title('Test Set - Actual vs Predicted PM2.5')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}