# Forecasting PM2.5 Air Pollution in Potsdam, Germany

This project aims to forecast PM2.5 air pollutant levels in Potsdam, Germany, using European Environment Agency data. 
It consists of Jupyter notebooks and Python classes with methods dedicated to data processing, analysis, and forecasting models.

## Structure

The project is structured into the following directories:

* `src/data`: Contains the transformed data in csv format and raw data in parquet and csv formats
* `src/models`: Contains the best-performed models in pickle format
* `src/notebooks`: Contains Jupyter notebooks for data analysis, visualization, and model training
* `src`: Contains Python classes and methods for data processing, model definition, training, tuning, evaluations, and forecasting

## Notebooks

The notebooks are organized into the following categories:
* `irp_pm_eda.ipynb`: Data preprocessing, transformation and exploratory data analysis, time series decomposition, feature engineering, and visualization
* `irp_pm_baseline.ipynb`: Naive approach baseline/benchmark model for hourly, daily, weekly, and monthly data, train/evolve 
* `irp_pm_traditional.ipynb`: Traditional machine learning regression models (SARIMAX, MLR, SVR) for hourly, daily, weekly, and monthly data, train/evolve and tuning
* `irp_pm_ensemble.ipynb`: Ensemble models (Random Forest, Gradient Boosting, Hist-Gradient Boosting, AdaBoost, XGBoost, CatBoost, Voting) for hourly, daily, weekly, and monthly data, train/evolve and tuning
* `irp_pm_dl.ipynb`: Deep learning models (DNN,LSTM, CNN) for hourly, daily, weekly, and monthly data, train/evolve and tuning

## Python Classes and Methods

The `src` directory contains the following Python classes and methods:
* `eda.py`: Data preprocessing, transformation and exploratory data analysis
* `model_base.py`: Contains methods for data transformation, helper methods and visualization
* `traditional.py`: Contains methods for traditional model for hourly, daily, weekly, and monthly data to train/evolve and tuning
* `ensemble.py`: Contains methods for ensemble models for hourly, daily, weekly, and monthly data to train/evolve and tuning
* `deep_learning.py`: Contains methods for deep learning models for hourly, daily, weekly, and monthly data to train/evolve and tuning
* `init.py`: Contains the initialization of the package

## Data

This project uses the European Environment Agency's air quality data for Potsdam, Germany. 
The data is available in the `data` directory and is organized into the following files:
* /parquet: Contains the raw data in parquet format
* /csv: Contains the raw data in csv format
* Transformed data in csv format is also available in the `data` directory


## Best Performed Models

The best-performed models are saved in the `models` directory. The models are saved in the following formats:
* .pkl: Python pickle format


Best models:
* `cnn_model_H.pkl`: The best-performed model for hourly data
* `svr_model_D.pkl`: The best-performed model for daily data
* `voting_model_W.pkl`: The best-performed model for weekly data
* `svr_model_M.pkl`: The best-performed model for monthly data

*** Because the models are large, they are not all included in the repository. However, they can be generated by running the notebooks in the `notebooks` directory.


## Prerequisites

Before you begin, ensure you have the following installed:

- Anaconda or Miniconda
- Python 3.8 or later
- Jupyter Notebook


## Conda Environment Setup

To create a conda environment with the required dependencies, run the command:

``` conda env export --no-builds | grep -v "prefix" > environment.yml```

activate the environment:

``` conda activate irp-air-pollution```


## Build and Run

To install required python libraries, run the command:  

``` pip install -r requirements.txt ```

To run the Jupyter notebooks, execute the command:

``` jupyter notebook ```


## Usage
This project is structured into Jupyter notebooks and Python classes that perform various tasks related to the forecasting of PM2.5 levels. 

* Jupyter Notebooks: Provide step-by-step guides for data analysis, visualization, and model tuning and training. Navigate through the notebooks to understand the workflow.

* Python Classes and Methods: Contain reusable code for data preprocessing, model definition, training , tuning, evaluations, and forecasting. These are used within the notebooks but can also be imported into other Python scripts if needed.

## Running Notebooks
After launching Jupyter Notebook, navigate to the directory containing the notebooks. Open the desired notebook by clicking on its name.
Run the notebook cells in sequence by pressing **Shift + Enter** or using the **"Run"** button in the toolbar.