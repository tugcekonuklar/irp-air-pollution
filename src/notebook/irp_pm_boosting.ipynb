{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Postdam PM2.5 Ensemble Boosting  Forcasting \n",
    "\n",
    "* Between 2013 and 2023, data collected by DEBB021 was used.\n",
    "* To increase the accuracy of PM2.5 data estimation, NO2, O3, SO2, PM10 pollutant gas data accepted by the EEA was added.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import src\n",
    "import model_base as mb\n",
    "import boosting as bo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "\n",
    "* Load Data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df= mb.get_cleaned_datetime_df()\n",
    "mb.set_start_date_time_index(df)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting Data \n",
    "\n",
    "Train, Validation and Test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = mb.split_data(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract the features\n",
    "X_train, X_val, X_test = mb.extract_features(train_data, validation_data, test_data)\n",
    "# Extract the target variable\n",
    "y_train, y_val, y_test = mb.extract_target(train_data, validation_data, test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradiend Boosting\n",
    "\n",
    "\n",
    "Gradient Boosting is an ensemble machine learning algorithm that combines the predictions of sequentially trained weak learners, typically decision trees, to create a strong model. It corrects its predecessors' errors by focusing on challenging cases. Optimization is guided by gradient descent, adjusting each tree's contribution using a learning rate. It's effective for both regression and classification, offering high predictive accuracy and handling feature importance well. Popular variants like XGBoost, LightGBM, and CatBoost provide faster and more scalable implementations. Despite its strengths, Gradient Boosting can be computationally intensive and may overfit if not carefully regularized."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bo.train_and_evolve(df, 'gradient_boosting')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ada Boosting\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bo.train_and_evolve(df, 'adaboost')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# histogram_gradient_boosting\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bo.train_and_evolve(df, 'histogram_gradient_boosting')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bo.train_and_evolve(df, 'xgboost')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost\n",
    "\n",
    "CatBoosting RegressionÂ¶\n",
    "CatBoost (Categorical Boosting) is an open-source gradient boosting library, developed at Yandex, that excels at working with categorical features without the need for extensive pre-processing. It uses a special algorithm to process categorical values and incorporates them into decision trees, which are then combined into a powerful ensemble model.\n",
    "\n",
    "One of the standout features of CatBoost is its innovative handling of categorical data. Traditionally, machine learning models require categorical variables to be transformed into numerical format, which is typically done through one-hot encoding or label encoding. However, CatBoost automates this process by applying an efficient encoding scheme that uses statistical information from the categories to improve performance.\n",
    "\n",
    "CatBoost also prioritizes speed and efficiency. It's designed to be fast even on large datasets and has GPU support for even quicker training. Additionally, it provides overfitting prevention mechanisms with its novel ordered boosting technique, which is a permutation-driven alternative to the classic gradient boosting method.\n",
    "\n",
    "Furthermore, CatBoost comes with a user-friendly interface and is compatible with both Python and R. It offers several parameter tuning options to improve model performance and can handle missing values internally. Its robustness and ease of use make it a popular choice for both regression and classification problems across various fields, from finance to biology."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bo.train_and_evolve(df,'catboost')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HyperPramater Tuning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best_gb_estimater_model = bo.tune_and_evaluate_boosting(df, 'gradient_boosting')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best_ada_gb_estimater_model = bo.tune_and_evaluate_boosting(df, 'adaboost')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}